---
output:
  pdf_document:
    fig_caption: true
---
## Challenge: Validating Detection

The current age in Western society is replete with digital services, and in many ways, participating in that society means using those services.  The nature of these services means using them produces inevitable contrails - not necessarily interpretable ones, but some tracks nonetheless.  For a variety of reasons, individuals or groups may prefer their relationships remain hidden, but they will still need or want to use these services.  Similarly, various organizations--both state actors and private businesses--desire to detect these groups, have access to these digital footprints, and will apply various techniques to identify the covert groups.

The problem at this point is a classic challenge in science: we can compute something which think may be useful, and to determine if it is useful that computation must be compared to the appropriate evidence, but the appropriate evidence is obscured by the nature of the phenomena.  We want to find covert groups, but the definition of covert groups is to be hidden.  We might conclude ex post that our approach found a covert group, but perhaps it was chance rather than design.  Perhaps we did not find a real covert group, just a random sample of the population.  Perhaps we did find a covert group, but missed many others.

In an absolute sense, these form an intractable epistemic problem.  But can be pragmatic: we can propose our best-guesses about how these covert groups might behave (or at least, the range of their behavior that we are willing to concern ourselves with), we can synthesize groups that behave that way and add those digital trails to real datasets, and then we can see how our algorithms perform.  This is an imperfect validation (we may not simulate a real possibility, we may simulate the behavior poorly, *etc.*), but is far superior post hoc storytelling.

In this work, we outline steps for a basic validation (sensitivity and specificity only) and apply it to a relatively unsophisticated detection scheme (two-stage social network community membership).  Clearly, different entities may have different validation outcomes they prefer, and more or less sophisticated detection algorithms.  The problem remains: how well do they know when an approach works?

## Detection Overview

The goal of the framework is to assess the real-time performance of different detection algorithms.  How do we define a detection algorithm?  It needs only be something that will produce a measurable labeling of cover or not.  A simple series of uninformed coin flips, while not likely to be useful, would qualify as it provides clear outcomes.  In general, we anticipate that there is some information that a detector would find useful to inform its indications, so a detection algorithm is also capable of receiving input (other than the actual true labels, presumably).  The coin-flipping approach still meets this criteria: it simply ignores all input.  So we can think a bit more specifically: a detection algorithm is some process that receives the history of events up to a certain point and then predicts which people are and are not members of a covert group.

Depending on the data available and model representation, *membership* in the covert group could entail a variety of outcomes.  For our purposes, we will only be considering social network representations, and the natural measure of group membership is shared network community.  Communities in a social network model are precisely defined in different ways for different purposes, but these different exact definitions generally share a notion of members in a community tending to be connected to each other and not connected to individuals in other communities.

## The Spin-Glass Community Definition

Throughout, we will consider social network communities as detected by the *spin-glass* algorithm; these are not the only kinds of communities, but they are the only ones we will be looking at.  Spin-glass refers to a statistical physics phenomena, where components of a system have a property *spin* and those components prefer to align with adjacent components of the same spin and anti-align with those of different spin.  This preference is expressed in terms of energy, and spin-glass systems seek to minimize their energy.  Magnets obey, in a loose sense, this kind of organization, but a spin-glass permits an arbitrary number of poles.  The spin-glass communities of some network are the arrangement of individuals into communities, each assigned a unique spin, such that alignment and anti-alignment energies of connected individuals is minimal.  The spin-glass community definition supports weighted connections between individuals (both positive and negative, though we only use positive weights), essentially assigning more potential energy change to more heavily weighted edges.  The algorithm for detecting the appropriate number of and membership assignment to spin-glass communities is done by a simulated-annealing style algorithm.

## Community Persistence Measure

To demonstrate the framework, we consider a detection scheme based on persistent community co-membership.  The detection scheme considers regular intervals (15 or 30 days), and aggregates events (two users logging into the same hot spot at the same time, which we interpret to mean that those individuals are both physically located at the building associated with the hotspot) during a window of time (the past 15 or 30 days) which we translate into social networks of people (unique system user ids) and connections (number of co-location).  Using this weighted network, communities are established using the spin-glass community algorithm.  The edge weights between individuals are equal to the number of co-location appearances in that window.

We call these communities the "snapshot" communities.  For the next stage, we consider persistent co-membership in communities.  For this calculation, we consider a different connection graph, using an edge weight derived from $\delta_{ij}(t) = 1$ if individuals $i$, $j$ are in the same community in interval $t$ and 0 otherwise.  Then, the community persistence score is

$$
S_{ij}(t) = \delta_{ij}(t) + \sum_{n=1}^{t-1}\delta_{ij}(t-n)r^n
$$

where $r$ is a tunable decay rate.  We can use these persistence scores as edge weights, and compute *persistence communities*, again using the spin-glass community detection algorithm.

## Labeling Covert Groups

At each time interval, we want to label some subset of the population as covert group members.  As more information becomes available (*i.e.*, as we move to the next interval), we should update those labels.  The persistence communities should reflect evolving knowledge: as interactions grow and wane, the associated persistence communities also change.  For users that remain consistent members of small persistence communities, based on a prior belief on how covert groups should behave (relative loners, essentially, working in small groups), we want to mark individuals that appear in small groups as potential covert members.  To so, at each interval, we label all members of all persistence communities that fit our beliefs about the size of covert groups--more than 2 members, fewer than 30--as potential covert group members.

## Evaluating Persistence Communities

In our framework, we know the truth about the covert membership.  We can then consider how the approach of labeling all small persistence communities as "covert" fairs: how many members of the covert group appear in these groups versus how many are missed?  How many regular users are falsely implicated?  What is the trajectory of those rates?  How does the labeling perform relative to different covert group behaviors?

## General Approach to Validation: Background Plus Synthetic Perturbations

For relatively large empirical data sets, applying detection algorithms many times with many different synthetic populations, different detection thresholds, *etc.* is likely to be computationally prohibitive.  This problem is not necessarily reflected in real-world application of those approaches: since there will not be multiple sampling replicates for the purpose of validation, there will only be as many runs as are required to apply the approach once.  Additionally, the data will be arriving in smaller, real time chunks and the analysis time will be spread out to accommodate that.

To deal with this issue in the validation studies, however, we have taken the approach of (for a particular detection algorithm) pre-computing some measures on the background population without any synthetic group present.  Since analyzing the background population is by far the largest contributor to run time, this means that the majority of computation can be done once, and that introducing the synthetic activities and then applying the detection algorithm to them can be done as a relatively fast-running perturbation calculation.

## Background Data Set: The Population Using the Municipal Montreal Wi-Fi System

For our background, we use the usage data from the municipal Montreal Wi-Fi system.  That data comprises the series of login events by unique user id at unique location id, with start and end times.

For the detection algorithm we are considering, there are two stages of community detection.

The first is applied to snapshots of the data: *i.e.* in particular time interval, we look at all the overlapping (location and time) hot spot visits, and draw edges between them.  Using those edges (weighted in our approach by the number of overlapping interactions), the community structure can be computed: we call the resulting groups *snapshot communities*.  We use the spin-glass approach because it is available pre-implemented in a common graph analysis package (igraph) and it can use weighted edge information.

The second stage entails tracking shared community membership between individuals across intervals and scoring that co-membership between individuals.  These scores, when above a certain threshold, are treated as weighted edges between individuals, and again we can run a community structure computation: we call the resulting groups *persistence communities*.  We also use the spin-glass algorithm for this analysis.

For those stages in the detection algorithm, we can pre-compute both sets of communities for the background population absent the perturbing covert group.  The following figures show (Fig. X) the evolution of snapshot community count and size distribution and (Fig. Y) the evolution of persistence community counts and sizes distribution.

![Snapshot Community Counts](output/background-clusters/spin-glass/base_cluster_trends_count.png)

![Snapshot Community Size Distributions](output/background-clusters/spin-glass/base_cluster_trends_distro.png)

![Persistence Community Counts](output/background-clusters/spin-glass/pc_cluster_trends_count.png)

![Persistence Community Size Distributions](output/background-clusters/spin-glass/pc_cluster_trends_distro.png)

# Generating a Synthetic Group

To generate a synthetic population, first we choose a *template* (or templates) for the individuals in that population.  The template is a population from the background with shared properties.  In our analysis, we identified clusters of users (from the sub population of users that appeared for longer than 30 days in the time series) based on the hot spots they used, and those background clusters became the templates.

To create a synthetic individual from the template population, we sample (with replacement) members of that population.  The real members use locations from various clusters over their life.  We select locations from the same clusters (though not necessarily the same locations) for the synthetic individual to use over their life.  Finally, the synthetic individual adopts the real users observed behavior statistics (*e.g.*, waiting time distribution between hot spot access).

With the synthetic individuals created, we create a synthetic group by connecting the synthetic individuals and assigning that connection a meeting hotspot and frequency.  During the simulation, group members will occasionally meet in addition to their individual behavior.  These meetings happen according to the connection frequency and at the associated location.  The simplest group simply connects all members, assigns all those connections the same frequency and location.

# Perturbations

After running a synthetic group simulation, the next step is to integrate the group into the background for the detection analysis.  For that, we need to assign their persistence communities and therefore their snapshot communities first.

To assign snapshot communities, we generate the time and place overlap series for the synthetic group and the background, as well as the synthetic-to-synthetic overlaps (due to either group meetings or incidental overlaps).  We then consider interval slices of this series and look at the connections of the synthetic individuals to the identified background communities.  The spin-glass algorithm used for the background can consider individual vertices and determine the community around them.  So we recreate the network in that interval, and then detect the perturbed community for each synthetic individual.  However, these communities are unlikely to overlap exactly the pre-identified communities.  To resolve that, we assign synthetic individuals to the consensus snapshot community of the background individuals to which they are connected.

Now, we can perform the persistence scoring for the synthetic individuals.  Again, we can look at each interval with persistence score edges, use spin-glass on the adapted networks to determine the communities for the synthetic individuals, and finally determine the consensus persistence communities for these individuals.